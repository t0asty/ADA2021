{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data from the Web - ADA 2020 Tutorial\n",
    "\n",
    "#### What do you find in this Notebook?\n",
    "\n",
    "The purpose of the Notebook is to offer a **quick** overview on how to scrape a Web page. In details, we illustrate the two main libraries used for this purpose. Afterwords, we show how to retrieve data from the Web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web scraping libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fetch data from the Web with Python, you need to get use to two essential libraries:\n",
    "\n",
    " * [`Requests (HTTP)`](https://requests.kennethreitz.org/en/master/): get the `html` page to parse.\n",
    "\n",
    " * [`Beautiful Soup (HTML Parsing)`](https://www.crummy.com/software/BeautifulSoup/bs4/doc/): parse the `html` and extract data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have an error about missing modules, try this:\n",
    "```\n",
    "conda install requests\n",
    "conda install beautifulsoup4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a `get` request\n",
    "\n",
    "The [GET method](https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol#Request_methods) retrieves information from the server.\n",
    "\n",
    "We start scraping this website: https://httpbin.org/ - HTTP Request & Response Service. The website offers some useful endpoints [1] to check the content of our request. Some of them provide an 'echo service' that reply with the request received.\n",
    "\n",
    "[1] Endpoint is a web address (URL) at which clients of a specific service can gain access to it. By referencing that URL, clients can get to operations provided by that service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 1: Get request basics\n",
    "Here we show an example on how use a get request. In particular, you see that we can get different information about the response:\n",
    "\n",
    "* The status code [2] which tells us whether everything is fine and if the request worked\n",
    "* The headers\n",
    "* Body of the response (typically HTML for webpages or JSON/XML for web services)\n",
    "\n",
    "[2] Find the reminder of HTTP status codes [here](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes). Some typical codes are: **200 OK** (standard response for successful HTTP requests) and **404 Not Found** (the requested resource could not be found but may be available in the future)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** this is an echo service, what you see is what we sent to the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response status code: 200\n",
      "\n",
      "Response headers: {'Date': 'Mon, 11 Oct 2021 07:33:35 GMT', 'Content-Type': 'application/json', 'Content-Length': '33', 'Connection': 'keep-alive', 'Server': 'gunicorn/19.9.0', 'Access-Control-Allow-Origin': '*', 'Access-Control-Allow-Credentials': 'true'}\n",
      "\n",
      "Response body: {\n",
      "  \"origin\": \"178.195.212.72\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make the request\n",
    "r = requests.get('https://httpbin.org/ip') # /ip: Returns the requester's IP Address.\n",
    "\n",
    "print('Response status code: {0}\\n'.format(r.status_code))\n",
    "print('Response headers: {0}\\n'.format(r.headers))\n",
    "print('Response body: {0}'.format(r.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2: Parsing JSON string from the response\n",
    "\n",
    "If the body of the response is a JSON string, Requests offers a convenient way to parse the text and get a Python dictionary.\n",
    "\n",
    "Let's try to get the current time from here: http://worldtimeapi.org/api/timezone/Europe/Zurich â€“ a simple web service that returns the local-time for a given timezone as either JSON (by default) or plain-text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response body (parsed json):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'abbreviation': 'CEST',\n",
       " 'client_ip': '178.195.212.72',\n",
       " 'datetime': '2021-10-11T09:34:13.506390+02:00',\n",
       " 'day_of_week': 1,\n",
       " 'day_of_year': 284,\n",
       " 'dst': True,\n",
       " 'dst_from': '2021-03-28T01:00:00+00:00',\n",
       " 'dst_offset': 3600,\n",
       " 'dst_until': '2021-10-31T01:00:00+00:00',\n",
       " 'raw_offset': 3600,\n",
       " 'timezone': 'Europe/Zurich',\n",
       " 'unixtime': 1633937653,\n",
       " 'utc_datetime': '2021-10-11T07:34:13.506390+00:00',\n",
       " 'utc_offset': '+02:00',\n",
       " 'week_number': 41}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = requests.get('http://worldtimeapi.org/api/timezone/Europe/Zurich')\n",
    "\n",
    "print('Response body (parsed json):')\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 3: Including parameters into get request\n",
    "\n",
    "This time, the `url` has been slightly changed to include a parameter (key1).\n",
    "\n",
    "Remember that the with the GET method the parameters are part of the URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'args': {'key1': 'value1'},\n",
       " 'headers': {'Accept': '*/*',\n",
       "  'Accept-Encoding': 'gzip, deflate, br',\n",
       "  'Host': 'httpbin.org',\n",
       "  'User-Agent': 'python-requests/2.26.0',\n",
       "  'X-Amzn-Trace-Id': 'Root=1-6163e901-1eba55444ac433d5156ab81d'},\n",
       " 'origin': '178.195.212.72',\n",
       " 'url': 'https://httpbin.org/get?key1=value1'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = requests.get('https://httpbin.org/get?key1=value1')\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a `post` request\n",
    "The [POST method](https://en.wikipedia.org/wiki/POST_(HTTP)) requests that a web server accepts the data enclosed in the body of the request message, most likely for storing it.\n",
    "\n",
    "A POST request can have the paramenters in the body. Let's how to do this with Requests library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'args': {},\n",
       " 'data': '',\n",
       " 'files': {},\n",
       " 'form': {'key1': 'value1', 'key2': 'value2'},\n",
       " 'headers': {'Accept': '*/*',\n",
       "  'Accept-Encoding': 'gzip, deflate, br',\n",
       "  'Content-Length': '23',\n",
       "  'Content-Type': 'application/x-www-form-urlencoded',\n",
       "  'Host': 'httpbin.org',\n",
       "  'User-Agent': 'python-requests/2.26.0',\n",
       "  'X-Amzn-Trace-Id': 'Root=1-6163e914-1a06af876df1b0674c431e76'},\n",
       " 'json': None,\n",
       " 'origin': '178.195.212.72',\n",
       " 'url': 'https://httpbin.org/post'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "payload = {'key1': 'value1', 'key2': 'value2'}\n",
    "r = requests.post('https://httpbin.org/post', data=payload)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a request and extract the Page Title!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Send the request and get the `html`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html>\\n<html>\\n  <head>\\n  </head>\\n  <body>\\n      <h1>Herman Melville - Moby-Dick</h1>\\n\\n      <div>\\n        <p>\\n          Availing himself of the mild, summer-cool weather that now reigned in these latitudes, and in preparation for the peculiarly active pursuits shortly to be anticipated, Per'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Send the request\n",
    "r = requests.get('https://httpbin.org/html')\n",
    "r.text[:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Thus, we start to use our beloved `BeautifulSoup` to parse the HTML and we get the header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h1>Herman Melville - Moby-Dick</h1>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the header\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "soup.h1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get interesting data - DBLP\n",
    "\n",
    "*DBLP is a computer science bibliography website. Starting in 1993 at the University of Trier, Germany, it grew from a small collection of HTML files and became an organization hosting a database and logic programming bibliography site. DBLP listed more than 3.66 million journal articles, conference papers, and other publications on computer science in July 2016, up from about 14,000 in 1995.*\n",
    "\n",
    "<div align=\"right\">https://en.wikipedia.org/wiki/DBLP</div> \n",
    "\n",
    "We want to check the distribution of the publications by year of the president of EPFL - Martin Vetterli.\n",
    "\n",
    "First of all, let's check the page with the data we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'http://dblp.uni-trier.de/pers/hd/v/Vetterli:Martin'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The page is public and accessible with a browser using a simple GET:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(URL)\n",
    "page_body = r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the page content is downloaded and we can inspect the body of the response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html>\\n<html lang=\"en\">\\n<head><meta charset=\"UTF-8\" /><title>dblp: Martin Vetterli</title><link rel=\"home\" href=\"https://dblp.org\" /><link rel=\"search\" type=\"application/opensearchdescription+xml\" href=\"https://dblp.org/xml/osd.xml\" title=\"dblp search\" /><link rel=\"apple-touch-icon\" type=\"i'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_body[:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is pure HTML, and we need BeautifulSoup to parse the content. We can specify the parser we want to use html.parser, lxml, lxml-xml, xml, html5lib. Each of them has advantages and disadvantages - see [documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#installing-a-parser)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page_body, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the page is parsed and we can read the data we need!\n",
    "\n",
    "For example, let's get the title! Are we in the right page?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>dblp: Martin Vetterli</title>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes! And we can get the clean text without HTML tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dblp: Martin Vetterli'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.title.string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more complex query now! Let's find all the links in the page. \n",
    "\n",
    "In HTML a link is defined using the tag &lt;A&gt;, and BeautifulSoup offers an easy way to find them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The webpage cointains 13269 links...\n"
     ]
    }
   ],
   "source": [
    "all_links = soup.find_all('a')\n",
    "print('The webpage cointains {0} links...'.format(len(all_links)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... and 12354 of them point to external websites.\n"
     ]
    }
   ],
   "source": [
    "external_links = 0\n",
    "for link in all_links:\n",
    "    if(link.get('href') and not link.get('href').startswith('http://dblp.uni-trier.de/')\n",
    "       and link.get('href').startswith('http')):  # just an example, you may need more checks\n",
    "        external_links += 1\n",
    "\n",
    "print('... and {0} of them point to external websites.'.format(external_links))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move on. Now we want to extract the sections that contain the publication details.\n",
    "\n",
    "**The easiest way is to inspect the DOM of the web page with a browser.** Check with your browser how to isolate the portions of the page that represent publications. --- Task not in this Notebook ---\n",
    "\n",
    "Ok, each row is composed by a &lt;li&gt; tag and has a class called 'entry':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "publications_wrappers = soup.find_all('li', class_='entry')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the number of rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of items: {0}'.format(len(publications_wrappers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in publications_wrappers:\n",
    "    print(p.find('span', class_='title').text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publications_list = []\n",
    "for p in publications_wrappers:\n",
    "    title = p.find('span', class_='title').text  # get the title\n",
    "    authos_list = p.find_all('span', {'itemprop': 'author'})  # get the authors list\n",
    "    authors = [author.text for author in authos_list]  \n",
    "    year = p.find('span', {'itemprop': 'datePublished'}).text\n",
    "    publications_list.append({'title': title, \n",
    "                         'authors': authors, \n",
    "                         'year': int(year)})  # here you should validate the data\n",
    "\n",
    "publications = pd.DataFrame.from_dict(publications_list)\n",
    "publications.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publications.groupby('year')\\\n",
    "    .count()\\\n",
    "    .rename(columns = {'title':'count'})\\\n",
    "    .plot(y='count', kind='bar', grid=True, figsize=(10, 6), title='Data from: ' + URL)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
