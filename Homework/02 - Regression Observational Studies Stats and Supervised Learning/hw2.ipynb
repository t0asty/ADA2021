{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Homework 2 (HW2)\n",
    "\n",
    "---\n",
    "By the end of this homework we expect you to be able to:\n",
    "1. Preprocess data and make it amenable to statistical analysis and machine learning models;\n",
    "2. Train and test out-of-the-box machine learning models in Python;\n",
    "3. Carry out statistical hypothesis testing;\n",
    "4. Carry out simple multivariate regression analyses;\n",
    "5. Use techniques to control for covariates;\n",
    "\n",
    "---\n",
    "\n",
    "## Important Dates\n",
    "\n",
    "- Homework release: Fri 12 Nov 2021\n",
    "- **Homework due**: Fri 26 Nov 2021, 23:59\n",
    "- Grade release: Fri 03 Dec 2021\n",
    "\n",
    "---\n",
    "\n",
    "##  Some rules\n",
    "\n",
    "1. You are allowed to use any built-in Python library that comes with Anaconda. If you want to use an external library, \n",
    "you may do so, but must justify your choice.\n",
    "2. Make sure you use the `data` folder provided in the repository in read-only mode. (Or alternatively, be sure you \n",
    "don’t change any of the files.)\n",
    "3. Be sure to provide a textual description of your thought process, the assumptions you made, the solution you \n",
    "implemented, and explanations for your answers. A notebook that only has code cells will not suffice.\n",
    "4. For questions containing the **/Discuss:/** prefix, answer not with code, but with a textual explanation\n",
    " (**in markdown**).\n",
    "5. Back up any hypotheses and claims with data, since this is an important aspect of the course.\n",
    "6. Please write all your comments in English, and use meaningful variable names in your code. Your repo should have a \n",
    "single notebook (plus the required data files) in the *master/main* branch. If there are multiple notebooks present, \n",
    "we will **not grade** anything.\n",
    "7. We will **not run your notebook for you**! Rather, we will grade it as is, which means that only the results \n",
    "contained in your evaluated code cells will be considered, and we will not see the results in unevaluated code cells. \n",
    "Thus, be sure to hand in a **fully-run and evaluated notebook**. In order to check whether everything looks as intended, you can check the rendered notebook on the GitHub website once you have pushed your solution there.\n",
    "8. In continuation to the previous point, interactive plots, such as those generated using `plotly`, should be **strictly avoided**!\n",
    "9. Make sure to print results or dataframes that confirm you have properly addressed the task.\n",
    "\n",
    "---\n",
    "\n",
    "## Context\n",
    "\n",
    "Congratulations! You have just been hired as a data scientist at *Piccardi Music,* a promising new music label created by a mysterious Italian disc jockey \"*Signor Piccardi*\". The company hired you to carry out a variety of data-related tasks, which will be explained in further detail below.\n",
    "\n",
    "---\n",
    "\n",
    "## The data\n",
    "\n",
    "For this homework you will use a dataset of 18,403 music reviews scraped from Pitchfork¹, including relevant metadata such as review author, review date, record release year, review score, and genre, along with the respective album's audio features pulled from Spotify's API. The data consists of the following columns:\n",
    "\n",
    "| Column   | Description  |\n",
    "|----------|:-------------|\n",
    "| `artist`           | The name of the artist who created the album being reviewed. |\n",
    "| `album`            | The name of the album being reviewed. |\n",
    "| `recordlabel`      | The name of the record label(s) who published the album. |\n",
    "| `releaseyear`      | The year that the album was released. |\n",
    "| `score`            | The score given to the album by the reviewer on a scale of 0.0 to 10.0. |\n",
    "| `reviewauthor`     | The name of the author who reviewed the album. |\n",
    "| `genre`            | The genre assigned to the album by Pitchfork. |\n",
    "| `reviewdate`       | The date that the review was published.  |\n",
    "| `key` | The estimated overall musical key of the track. Integers map to pitches using standard Pitch Class notation (e.g., 0 = C, 2 = D, and so on) |\n",
    "| `acousticness` | A confidence measure from 0.0 to 1.0 of whether an album is acoustic. 1.0 represents high confidencethat the album is acoustic. |\n",
    "| `danceability` | How suitable an album is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 1.0 is most danceable.\n",
    "| `energy` | A perceptual measure of intensity and activity, from 0.0 to 1.0, where 1.0 represents high energy. Metal is often high energy. |\n",
    "| `instrumentalness` | Predicts whether an album contains no vocals, from 0.0 to 1.0. The closer to 1.0, the more likely the album contains no vocals. |\n",
    "| `liveness` | Detects the presence of an audience, from 0.0 to 1.0. Scores greater than 0.8 indicate a strong likelihood the album is live. |\n",
    "| `loudness` | The overall loudness of the album in decibels (dB). |\n",
    "| `speechiness` | Measures the presence of spoken words in an album on a scale from 0.0 to 1.0. Scores higher than 0.66 indicate an album made entirely of spoken words, while scores below 0.33 indicate music and other non-speech-like elements. |\n",
    "| `valence` | A measure from 0.0 to 1.0 describing the musical positiveness conveyed by an album, where values closer to 1.0 indicate more positive sounds. |\n",
    "| `tempo` | The overall estimated tempo of an album in beats per minute (BPM). |\n",
    "\n",
    "¹Pinter, Anthony T., et al. \"P4KxSpotify: A Dataset of Pitchfork Music Reviews and Spotify Musical Features.\" Proceedings of the International AAAI Conference on Web and Social Media. Vol. 14. 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE THIS IF YOU NEED/WANT TOO\n",
    "\n",
    "# pandas / numpy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# datetime operations\n",
    "from datetime import datetime\n",
    "\n",
    "# ttest and euclidean distance\n",
    "from scipy.stats import ttest_ind\n",
    "from scipy.spatial.distance import seuclidean\n",
    "\n",
    "# linear fit using statsmodels\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# good ole sklearn\n",
    "from sklearn.metrics import euclidean_distances, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# displaying markdown strings\n",
    "from IPython.display import display, Markdown, Latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Part 1: Will this album be a hit?\n",
    "\n",
    "The first project you embark on in your new job is to build a regressor to predict whether an album will be well received or not. According to *Signor Piccardi* (your boss), this algorithm may eventually be helpful in forecasting the success of albums produced by *Piccardi Music*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1 (Initial analyses — 10 pts)**\n",
    "\n",
    "As a good data scientist, the first thing you do is to have a good look at the data that was handed to you.\n",
    "\n",
    "1. Load the data using `pandas`. Identify and remove duplicate reviews, i.e., two reviews with albums by the same band with the same name (keep the first occurrence). Print the number of rows in your dataframe.\n",
    "\n",
    "2. Plot the distribution of album release years and the average score of albums per year.\n",
    "\n",
    "3. For numerical columns, calculate the mean, median, minimum value and maximum value. Additionally, plot the distribution for all the numerical columns in a single image (possibly with multiple subplots). Your image should be at most 14 inches wide by 14 inches long.\n",
    "\n",
    "3. For categorical columns, list how many different values there are in each column. If there are less than 10 distinct values for a category, print them all. For the `genre` column, assign the value `'Other'` for albums where the value is either `'none'` or `NaN`.\n",
    "\n",
    "5. **Discuss:** This dataset was built with *found data*—i.e., the Pitchfork reviews were not made with the goal of training a machine learning model. Grounded on the previous analyses and in Pitchfork's [Wikipedia page](https://en.wikipedia.org/wiki/Pitchfork_(website)), point **three** (exactly!) ways in which this data may not be representative of music albums in general due to the way the data was collected. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### **Task 2 (Pre-processing and pipeline code — 12 pts)**\n",
    "\n",
    "Next, you decide to prepare the code that will help you in training your machine learning models. Also, you implement a simple baseline. For this task, **unless otherwise stated** you must implement functions yourself, instead of relying on `scikit-learn` (you can use `numpy` or `pandas`, though!).\n",
    "\n",
    "1. For each possible value in the `genre` column, create a new column called `{genre}_onehot` (e.g., for `genre=jazz`, create `jazz_onehot`). Collectively, these new columns should \"one hot-encode\" the genre column—for instance, if for a given album the `genre` is filled with the value `jazz`, the `jazz_onehot` column should equal 1 and all other `{genre}_onehot` columns should equal 0. \n",
    "\n",
    "2. Create a function `numpy_helper(df, cols)` to obtain a `numpy.array` out of your `dataframe`. The function should receive a dataframe `df` with N rows and a list of M columns `cols`, and should return a `np.array` of dimension (NxM).\n",
    "\n",
    "3. For each album, build an array of features `X` containing all genre-related one-hot features, and an array of outcomes `y` containing scores. Using the function [`sklearn.model_selection.train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) with `random_state=123`, split the data into a train set containing 70% of all data, and a test set containing the remaining 30%.\n",
    "\n",
    "4. Create your own baseline regressor. Using the training data (in the training stage), your regressor should estimate the average score for all albums. Then, for the test data (in the prediction stage), your classifier should always output the average score (computed on the training data).\n",
    "\n",
    "5. Calculate the [coefficient of determination ($R^2$)](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html) of your baseline on the test data. **You are allowed to use the `sklearn` implementation here.**\n",
    "\n",
    "6. **Discuss:** Your train-test split randomly selected 70% of all data for the training set. Why is this a problem for the broader task of predicting whether a future album will be successful or not?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3 (Regression — 14 pts)**\n",
    "\n",
    "Finally, you get down to business and train your regression models.\n",
    "\n",
    "1. Build a Linear Regression model (use `sklearn`) that predicts the outcome `score` using the features \"releaseyear\",  \"key\", \"acousticness\", \"danceability\", \"energy\", \"instrumentalness\", \"liveness\", \"loudness\", \"speechiness\", \"valence\", \"tempo\" and the one-hot encoded genre-related columns.  Using a 70/30 train-test split similar to what you did in task two (hereinafter referred to as \"the random split\", use the same random seed, `random_state=123`), report the $R^2$ for the testing set.\n",
    "\n",
    "2. Create an alternate train-test split (hereinafter referred to as \"the longitudinal split\") where you train with data from albums released before 2000 and test with data from 2003 and after. Report the $R^2$ for the testing set using the same model you developed for the previous question. **Discuss:** give the correct interpretation of $R^2$ value for the longitudinal split.\n",
    "\n",
    "3. For a given entry $X$ your model outputs a predicted score $Y'$. The difference between the real score $Y$ and the predicted score $Y'$ is called the \"residual\". Considering the model trained in 3.2, plot the distribution of your residuals for the test set. Additionally, estimate what is the probability that your score prediction (from 3.2) is off by more than 2-points? Provide bootstrapped confidence intervals for your answer.\n",
    "\n",
    "4. Experiment with training a different regressor, a Gradient Boosting Regressor. This regressor is related to the Boosted decision trees that you have seen in class. This model performs extremely well for a variety of tasks and is often used in machine learning competitions for tabular data (e.g., on Kaggle). Train the regressor using the longitudinal split and the same features as in 3.2, use the default hyperparameters. Report the $R^2$ for the testing set. \n",
    "\n",
    "5. **Discuss:** Hypothesize a reason for the difference in performance between the Linear regression and the Gradient Boosting Regressor.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4 (Are we solving the correct problem? — 16 pts)**\n",
    "\n",
    "All your efforts so far have assumed that decisions are taken at the \"album\" level, which is often not the case for bands with multiple albums. In those cases, it could be interesting to predict what is the success of a given band album given the features of the album **and of previous albums**.\n",
    "\n",
    "1. Create a new dataframe that contains one row per band with more than 1 album. This dataframe should have the same columns as the data provided to you, considering the latest album of the respective band (note that this is determined by the release year of the album, not the date when it was reviewed). Additionally, for each feature considered in Task 3.1 (including the one-hot encoded features), create an additional column post-fixed with `_previous` (e.g., `danceability_previous`). These columns should contain the average values for all of the band's previous albums. Also, create a column `score_previous` with the average score of previous albums. Print the number of rows in the dataframe as well as the name of the columns.\n",
    "\n",
    "2. Train a Gradient Boosting Regressor considering all features created in Task 4.1 (note that `score` is the outcome and everything else is a feature, including `score_previous`). Use the 70/30 random train-test split, the default hyperparameters, and report the $R^2$ for the testing set. \n",
    "\n",
    "3. Can hyperparameter tuning improve your model? Write modular code (i.e., a function) to divide your training data into $N$ folds and perform cross-validation. Experiment tuning two hyperparameters of the Gradient Boosting Regressor: `n_estimators` and `learning_rate`. For each possible combination of the two hyperparameters (see below for the range of values that you should try for each hyperparameter), train your model in a cross-validation setup with $N=20$ folds. Report the mean  $R^2$ along with the 90% CI for each scenario. \n",
    "    - n_estimators $ \\in  \\{ 100, 200, 300, 400\\}$\n",
    "    - learning_rate $ \\in  \\{ 0.1, 0.05, 0.01\\}$.\n",
    " \n",
    " With the best hyperparameters obtained, train your model with the entire training set and report the $R^2$ on the testing set.\n",
    " \n",
    "4. **Discuss:** How do these results compare with the previous setup (the scenario considered in **Task 3.4**)? Point out two reasons why it is hard to compare the results obtained in 4.3 and 3.4 at face value? How would you fairly compare the two different setups?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Second Album Syndrome\n",
    "\n",
    "Your second project at *Piccardi Music* is to shed light on one of the business's oldest enigmas: the \"second album syndrome.\" In a nutshell, the \"second album syndrome\" is a theory that states that the second album of a band always sucks. ([Related read](https://tvtropes.org/pmwiki/pmwiki.php/Main/SophomoreSlump))\n",
    "\n",
    "Assume—for the purpose of this task—that the Pitchfork data contains all albums for all artists it covers (even though this might not be true in reality)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Task 5 (Preliminary analyses — 8 pts)**\n",
    "\n",
    "You begin by carrying out some preliminary data processing and analyses.\n",
    "\n",
    "1. Create a new dataframe (from the original data with duplicates dropped, the same as you obtained after finishing Task 1.1) containing only albums from artists with two or more albums and where the release year is not empty.\n",
    "2. Create a new column `album_number` which indicates how many albums the artist has produced before this one (before the second album, the artist has already produced one album).\n",
    "3. Calculate the mean and the standard error fo the mean of the scores of the first and second albums in the dataset. Additionally, plot the two distributions. \n",
    "4. Use an appropriate method to determine if the difference in means of 1st and 2nd albums is statistically significant?\n",
    "5. **Discuss:** Do these analyses suggest that the \"second album syndrome\" exists?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 6 (Regression analysis — 20 pts)**\n",
    "\n",
    "Next, you proceed to examine some hypotheses about the \"second album syndrome\" using a regression framework. Namely:\n",
    "\n",
    "- The **time spent hypothesis:**  the first album usually has a couple of years of development under its belt and plenty of trial and error from live concerts to help the band determine what does or doesn't work. The second album, on the other hand, is often made in a rush.\n",
    "\n",
    "- The **style change hypothesis:** bands often try to change their style after their first album. This change is not always welcomed by the listeners.\n",
    "\n",
    "1. Create a new dataframe containing one row per 1st-2nd album pair. The dataframe should contain rows:\n",
    "    - `score_diff`: the difference in scores between the second and the first album (second - first).\n",
    "    - `time_diff`: the number of days elapsed between the first and the second album.\n",
    "    - `did_style_change`: a dummy variable that indicates whether the style of the music has changed. To obtain it, first, calculate the [standardized euclidean distance](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.seuclidean.html) of music-related numerical features¹ between the second and the first album. Second, assign 1 to the 20% most distant 1st-2nd album pairs and 0 to all others.\n",
    "2. Fit a linear regression using `statsmodels` with this dataframe. Your regression should consider only an intercept, i.e., `\"score_diff ~ 1\"`.\n",
    "3. **Discuss:** Interpret the $R^2$ in your regression here. Does this analysis confirm what you observed in Task 5? Why?\n",
    "4. Include the `time_diff` and `did_style_change` as covariates in your model. Fit the regression again and report the summary of your model. \n",
    "5. **Discuss:** Interpret the coefficients `time_diff` and `did_style_change`. Carefully explain whether they provide evidence towards each of the aforementioned hypotheses? Do they rule out other reasons that may cause the \"second album syndrome effect\"?\n",
    "6. Create a new column called `time_diff_standardized`. It should be a standardized version of the `time_diff` column. Repeat the regression done in 6.4 using the `time_diff_standardized` column instead of the `time_diff` column.\n",
    "7. **Discuss:** Explain how the interpretation of the coefficients associated with this new column `time_diff_standardized` differ from its non-standardized version \n",
    "`time_diff`?\n",
    "\n",
    "--- \n",
    "\n",
    "**¹** Music related numerical features are:  \"key\", \"acousticness\", \"danceability\", \"energy\", \"instrumentalness\", \"liveness\", \"loudness\", \"speechiness\", \"valence\", and \"tempo\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Task 7 (Sanity checks — 6 pts)**\n",
    "\n",
    "You decide to perform a few last sanity checks for your analysis.\n",
    "\n",
    "1.  **Discuss:** If the Second Album Syndrome existed, i.e., something was special about the second album (as in if it was bad for a very particular reason that afflicted album number 2 more than all others), what would you expect to happen to the mean score of the third album?\n",
    "2. Using the dataset you created in Task 5, calculate the mean and the standard error of the mean for the 1st, 2nd, 3rd, and 4th albums. Test whether the difference between the average score of the second and the third album is statistically significant.\n",
    "3. **Discuss:** Does this suggest that the Second Album Syndrome exists?\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 8 (Eureka — 14 pts)**\n",
    "\n",
    "Your boss, *Signor Piccardi*, proposes that you carry out a simulation to make things clearer. Assuming that:\n",
    "\n",
    "\n",
    "- Each band $i$ has a \"talent\" $\\mu_i$ , which is uniformally distributed between 2 and 8, i.e., $\\mu_i \\sim U_{[2,8]}$.\n",
    "- When a band $i$ produces an album $j$, it has quality $s_j$. This score is normally distributed with mean $\\mu_i$ and standard deviation $1$, i.e., $s_j \\sim N(\\mu_i, 1)$\n",
    "- Talents are independent and identically distributed random variables.\n",
    " \n",
    "Carry out the following simulation:\n",
    "\n",
    "- Create 1000 hypothetical bands with intrinsic talents $\\mu_i \\sim U_{[2,8]}$ for $i \\in [1,1000]$.\n",
    "- Have each hypothetical band create a hypothetical album.\n",
    "- Discard all bands whose albums received a score smaller than 6.\n",
    "- For each of the remaining bands, create two additional albums.\n",
    "\n",
    "Analyzing the scores obtained in this simulation, provide a coherent explanation for the  scores obtained in Task 7.2. \n",
    "\n",
    "--- \n",
    "\n",
    "**Hint:** You can use numpy to sample random variables (e.g. [numpy.random.normal](https://numpy.org/doc/stable/reference/random/generated/numpy.random.normal.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
